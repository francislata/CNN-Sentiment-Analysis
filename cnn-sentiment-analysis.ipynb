{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Sentiment Analysis of Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this project is to implement Yoon Kim's [paper](https://arxiv.org/abs/1408.5882) for using a convolutional neural network to classify the sentiment of movie reviews. In this paper, they have shown that even a simple convolutional neural network can achieve state of the art results, with a pretrained word embedding like **word2vec**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My approach for this project is to incrementally build a CNN with randomly-initialized, non-static, word embeddings and work towards using a pre-trained word embedding. Once that's achieved, I am planning to add more \"channels\" (i.e., a static and non-static word embeddings) just like in Kim's paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have similarly followed what Kim's paper setup under section *3.1: Hyperparameters and Training*. I have ensured that validation set is using 10% of the training dataset (since there is no dedicated validation set). Also, the batch size used is 50. All filters have the sizes between 3 to 5 with 100 channel outputs. \n",
    "\n",
    "The only difference is that I am using the model with the highest seen validation accuracy after running the specified amount of epochs. This is different from the idea that early stopping is employed in Kim's paper.\n",
    "\n",
    "Lastly, the **GloVe** word embeddings are used instead of **word2vec**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several packages needed to be imported to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data import BucketIterator\n",
    "from torchtext.data import Field, LabelField\n",
    "import torchwordemb\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `torchtext` makes the importing of our files much easier for the IMDB review dataset as it provides the `IMDB` class for us to work with. Similarly, integrating other datasets with `torchtext` is pretty straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several helper functions will be made for better organization of loading the IMDB movie reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fields(input_tokenizer=\"spacy\"):\n",
    "    \"\"\"Creates the input and output fields for extracting the reviews and the labels\n",
    "    \n",
    "    Args:\n",
    "        input_tokenizer: The tokenizer to use for the input that is a supported tokenizer or custom function\n",
    "    \n",
    "    Returns:\n",
    "        The input and output fields to use\n",
    "    \"\"\"\n",
    "    input_field = Field(tokenize=input_tokenizer)\n",
    "    output_field = LabelField(tensor_type=torch.FloatTensor)\n",
    "    \n",
    "    return input_field, output_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_load_dataset(input_field, output_field, dataset_path=\".data\", validation_set_split_ratio=0.0, input_vectors_cache=\"vectors\", input_vectors=None):\n",
    "    \"\"\"Loads the dataset and builds the vocabularies and returns the appropriate datasets\n",
    "    \n",
    "    Args:\n",
    "        input_field: The input field to put the review for each example\n",
    "        output_field: The output field to put the label of each example onto\n",
    "        dataset_path: The path to extract and save the dataset to\n",
    "        validation_set_split_ratio: A ratio to split the training set into training and validation sets by\n",
    "        input_vectors_cache: The location to store the pre-trained word embeddings\n",
    "        input_vectors: The name of the pre-trained word embeddings to use\n",
    "        \n",
    "    Returns:\n",
    "        The training, optional validation, and test set iterators\n",
    "    \"\"\"\n",
    "    training_dataset, test_dataset = IMDB.splits(input_field, output_field, root=dataset_path)\n",
    "    if validation_set_split_ratio > 0.0:\n",
    "        training_dataset, validation_dataset = training_dataset.split(split_ratio=1.0 - validation_set_split_ratio)\n",
    "        \n",
    "    if input_vectors is None:\n",
    "        input_field.build_vocab(training_dataset)\n",
    "    else:\n",
    "        input_field.build_vocab(training_dataset, vectors=input_vectors, vectors_cache=input_vectors_cache)\n",
    "        \n",
    "    output_field.build_vocab(training_dataset)\n",
    "    \n",
    "    if validation_dataset is None:\n",
    "        return BucketIterator.splits((training_dataset, test_dataset), sort_key=lambda batch: IMDB.sort_key(batch), batch_size=50, repeat=False)\n",
    "    else:\n",
    "        return BucketIterator.splits((training_dataset, validation_dataset, test_dataset), sort_key=lambda batch: IMDB.sort_key(batch), batch_size=50, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `split_ratio` is the ratio of how much data goes to the training dataset. Hence, the `validation_set_split_ratio` must be subtracted from `1.0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the dataset will be loaded into `dataset_path` and will build the vocabulary of the input and output fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "input_field, output_field = create_fields()\n",
    "\n",
    "kwargs = {\"dataset_path\": \"data\", \"validation_set_split_ratio\": 0.1}\n",
    "training_dataset, validation_dataset, test_dataset = prepare_and_load_dataset(input_field, output_field, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with the different model architectures, it is ideal to setup our helper functions that I will be using to train, validate, and test our model architectures. Two functions will be made: one for training the model and one for evaluating the model's performance. Note that _accuracy_ is the metric that I will consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, loss_function, dataset, optimizer=None, is_training_model=True):\n",
    "    \"\"\"Runs the model given its dataset and using the loss function and optimizer\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        loss_function: The loss function to use\n",
    "        dataset: The dataset\n",
    "        optimizer: The optimizer to use when training the model\n",
    "        is_training_model: A flag indicating whether to run the model in training or evaluation mode\n",
    "        \n",
    "    Returns:\n",
    "        The average loss and accuracy for running through the dataset\n",
    "    \"\"\"\n",
    "    if is_training_model:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "        \n",
    "    accuracies = 0.0\n",
    "    losses = 0.0\n",
    "    \n",
    "    with torch.set_grad_enabled(is_training_model):\n",
    "        for example in tqdm_notebook(dataset):\n",
    "            if is_training_model:\n",
    "                model.zero_grad()\n",
    "        \n",
    "            inputs, targets = example.text, example.label\n",
    "        \n",
    "            predictions = model(inputs)\n",
    "            loss = loss_function(predictions, targets)\n",
    "            accuracy = calculate_accuracy(torch.sigmoid(predictions), targets)\n",
    "        \n",
    "            losses += loss.cpu().data.item()\n",
    "            accuracies += accuracy.cpu().data.item()\n",
    "        \n",
    "            if is_training_model:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "    return losses / len(dataset), accuracies / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, targets):\n",
    "    \"\"\"Calculates the accuracy given the predictions and targets\n",
    "    \n",
    "    Args:\n",
    "        predictions: The predictions made by the model\n",
    "        targets: The targets for the set of examples\n",
    "        \n",
    "    Returns:\n",
    "        The accuracy of the model\n",
    "    \"\"\"\n",
    "    return (predictions.round() == targets).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `run_model()` function will handle both training and evaluation phases for every epoch. The last function that needs to be created is the one responsible for running the model for a certain amount of epochs. Note that this function employs the optimization of saving the best parameters with the highest validation accuracy across all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model, loss_function, optimizer, training_dataset, test_dataset, validation_dataset=None, num_epochs=5):\n",
    "    \"\"\"Trains and evaluates the given model\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train and evaluate\n",
    "        loss_function: The loss function\n",
    "        optimizer: The optimizer to use when training the model\n",
    "        training_dataset: The dataset to use for training the model\n",
    "        test_dataset: The dataset to use for testing the model on unseen data\n",
    "        validation_dataset: The dataset to validate the model after training\n",
    "        num_epochs: The number of epochs to train and evaluate the model for\n",
    "    \"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        \n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_valid_accuracy = 0.0\n",
    "        \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        training_loss, training_accuracy = run_model(model, loss_function, training_dataset, optimizer=optimizer)\n",
    "        \n",
    "        print(\"[Training - Epoch {}] Loss: {:.2f}\".format(epoch, training_loss))\n",
    "        print(\"[Training - Epoch {}] Accuracy: {:.2f}\".format(epoch, training_accuracy))\n",
    "        \n",
    "        if validation_dataset is not None:\n",
    "            validation_loss, validation_accuracy = run_model(model, loss_function, validation_dataset, is_training_model=False)\n",
    "            \n",
    "            print(\"[Validation - Epoch {}] Loss: {:.2f}\".format(epoch, validation_loss))\n",
    "            print(\"[Validation - Epoch {}] Accuracy: {:.2f}\".format(epoch, validation_accuracy))\n",
    "            \n",
    "            if validation_accuracy > best_valid_accuracy:\n",
    "                best_valid_accuracy = validation_accuracy\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "    model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    test_loss, test_accuracy = run_model(model, loss_function, test_dataset, is_training_model=False)\n",
    "    \n",
    "    print(\"[Test] Loss: {:.2f}\".format(test_loss))\n",
    "    print(\"[Test] Accuracy: {:.2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: CNNRand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, I am going to create a convolutional neural network where the embeddings used for each word in every review is randomly initialized and will be learned during training. This is a baseline model that I am going to build upon. In particular, I am going to use the following layers: `Embedding, Conv2d, MaxPool1d, Linear, Dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNRand(nn.Module):\n",
    "    def __init__(self, num_vocab, embedding_dim, channel_out, filter_sizes, num_output, dropout_prob):\n",
    "        super(CNNRand, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_vocab, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, channel_out, filter_sizes[i]) for i in range(len(filter_sizes))])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * channel_out, num_output)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.t()\n",
    "        \n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        x_convs = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x_pools = [F.max_pool1d(x_conv, x_conv.size(2)).squeeze(2) for x_conv in x_convs]\n",
    "        x_cat = self.dropout(torch.cat(x_pools, 1))\n",
    "        x_fc = self.fc(x_cat).squeeze(1)\n",
    "        \n",
    "        return x_fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the values that we will be using as arguments to this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "CHANNEL_OUT = 100\n",
    "FILTER_SIZES = [(i, EMBEDDING_DIM) for i in range(3, 6)]\n",
    "NUM_OUTPUT = 1\n",
    "DROPOUT_PROB = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [len(input_field.vocab), EMBEDDING_DIM, CHANNEL_OUT, FILTER_SIZES, NUM_OUTPUT, DROPOUT_PROB]\n",
    "cnn_rand = CNNRand(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function that will be used is `BCEWithLogitsLoss` and the optimizer used is `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cnn_rand.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `CNNRand` model is going to be trained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb324cf771c4add845b1ec926c1750c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 1] Loss: 0.66\n",
      "[Training - Epoch 1] Accuracy: 0.61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d3733ee93b46dd93da2836c3c3bbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 1] Loss: 0.53\n",
      "[Validation - Epoch 1] Accuracy: 0.75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e7776e609e48cb9b656cc2486a1999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 2] Loss: 0.54\n",
      "[Training - Epoch 2] Accuracy: 0.73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b7e2df47b141c7871f65f60710e9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 2] Loss: 0.46\n",
      "[Validation - Epoch 2] Accuracy: 0.79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8904605c4c4bc18a1e8e2fd885a144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 3] Loss: 0.46\n",
      "[Training - Epoch 3] Accuracy: 0.78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ef059f23854e039a127595e58c8304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 3] Loss: 0.40\n",
      "[Validation - Epoch 3] Accuracy: 0.82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5ecf2a2cce4fad9e0a90ab5caa9512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 4] Loss: 0.40\n",
      "[Training - Epoch 4] Accuracy: 0.82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a6a89a73fb4ae2a1e7cce7390c2cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 4] Loss: 0.35\n",
      "[Validation - Epoch 4] Accuracy: 0.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cbfa1db4394e4c97f8cf796aad76a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 5] Loss: 0.34\n",
      "[Training - Epoch 5] Accuracy: 0.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffed2228fc6d4ff0aa1b158c30924879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 5] Loss: 0.34\n",
      "[Validation - Epoch 5] Accuracy: 0.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036dbd04d63e4c4aa7c77a95187c41a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test] Loss: 0.34\n",
      "[Test] Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "train_evaluate_model(cnn_rand, loss_function, optimizer, training_dataset, test_dataset, validation_dataset=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying out multiple filter sizes, this model has reached approximately 85% accuracy in the test set.\n",
    "\n",
    "Multiple filter sizes are not able to find the most important words that determine whether a movie review has a positive or negative sentiment. This is due to the randomly-initialized embedding matrix that is learned during training - which provides no information about each word in a sentence. I expect that using a pre-trained word embedding, like word2vec or GloVe, should give the model more information about the words and be able to detect the most important words for sentiment analysis.\n",
    "\n",
    "Finally, it is ideal to note that training and evaluating a CNN for sentiment classification is much faster than an RNN and that the accuracy is achieved in less than 10 epochs, based from my exploration in [here](https://gitlab.com/bigbawsboy/IMDB-sentiment-analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNNStatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, the `Embedding` layer will use the weights from a pre-trained word embedding. In particular, I will be using GloVe: a 100 dimension word embedding. This model is static because the embedding layer will not update its parameters during training while the rest of the layers will update their parameters.\n",
    "\n",
    "In order to support pre-trained word embeddings, the `create_fields()` function has been modified to accept a name of one of the supported pre-trained word embeddings and the directory to download the embeddings at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_field, output_field = create_fields()\n",
    "\n",
    "kwargs = {\"dataset_path\": \"data\", \"validation_set_split_ratio\": 0.1, \"input_vectors_cache\": \"vectors\", \"input_vectors\": \"glove.6B.100d\"}\n",
    "training_dataset, validation_dataset, test_dataset = prepare_and_load_dataset(input_field, output_field, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the implementation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNStatic(nn.Module):\n",
    "    def __init__(self, embedding_weights, num_vocab, embedding_dim, channel_out, filter_sizes, num_output, dropout_prob):\n",
    "        super(CNNStatic, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_vocab, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(embedding_weights)\n",
    "        self.embedding.requires_grad = False\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, channel_out, filter_sizes[i]) for i in range(len(filter_sizes))])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * channel_out, num_output)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.t()\n",
    "        \n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        x_convs = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x_pools = [F.max_pool1d(x_conv, x_conv.size(2)).squeeze(2) for x_conv in x_convs]\n",
    "        x_cat = self.dropout(torch.cat(x_pools, 1))\n",
    "        x_fc = self.fc(x_cat).squeeze(1)\n",
    "        \n",
    "        return x_fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters used for this model are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_WEIGHTS = input_field.vocab.vectors\n",
    "NUM_VOCAB = EMBEDDING_WEIGHTS.size(0)\n",
    "EMBEDDING_DIM = EMBEDDING_WEIGHTS.size(1)\n",
    "CHANNEL_OUT = 100\n",
    "FILTER_SIZES = [(i, EMBEDDING_DIM) for i in range(3, 6)]\n",
    "NUM_OUTPUT = 1\n",
    "DROPOUT_PROB = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that most of the hyperparameters are the same except for the embedding weights and its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [EMBEDDING_WEIGHTS, NUM_VOCAB, EMBEDDING_DIM, CHANNEL_OUT, FILTER_SIZES, NUM_OUTPUT, DROPOUT_PROB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670e70ab502b4084a18ef51482721383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 1] Loss: 0.46\n",
      "[Training - Epoch 1] Accuracy: 0.77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561b4237018a473a83f05f2bdefcbd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 1] Loss: 0.30\n",
      "[Validation - Epoch 1] Accuracy: 0.88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2341d37b9e904ed2b3e15215ad73debc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 2] Loss: 0.27\n",
      "[Training - Epoch 2] Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c050117a0b64d2abe4a5c32b129a6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 2] Loss: 0.27\n",
      "[Validation - Epoch 2] Accuracy: 0.88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b23b4e922034977a3ba749bf0faac9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 3] Loss: 0.17\n",
      "[Training - Epoch 3] Accuracy: 0.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd4e88d5c074ff6abc828df98aa9912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 3] Loss: 0.25\n",
      "[Validation - Epoch 3] Accuracy: 0.90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9df818c6db438486214e6a2c1133d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 4] Loss: 0.08\n",
      "[Training - Epoch 4] Accuracy: 0.97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44951a2ed0f94d49a6f4d9670f77a7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 4] Loss: 0.27\n",
      "[Validation - Epoch 4] Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69a53f9b3d7449c956d51d47a7554ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 5] Loss: 0.04\n",
      "[Training - Epoch 5] Accuracy: 0.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc6ab3a904f463a8cd42932a22c913f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 5] Loss: 0.32\n",
      "[Validation - Epoch 5] Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db9801204114a14a61a15d5c4bafc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test] Loss: 0.27\n",
      "[Test] Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "cnn_static = CNNStatic(*args)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cnn_static.parameters())\n",
    "\n",
    "train_evaluate_model(cnn_static, loss_function, optimizer, training_dataset, test_dataset, validation_dataset=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 5 epochs, this model is able to achieve an accuracy of 89% on the test set. I have used the same number of filter sizes as in the `CNNRand` model. Note that since the `Embedding` layer's parameters have not been learned during training, it could not adjust its information accordingly based on the context from the training dataset.\n",
    "\n",
    "By allowing the `Embedding` layer to adjust its parameters to learn more about the context of words in each example in the training dataset, the accuracy should increase much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: CNNNonStatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, the `Embedding` layer will adjust its parameters during the training phase of the model. The model architecture still stays the same. As a result, I will subclass the `CNNStatic` class and set `requires_grad` to `True`. Another approach to making this change is to change the initializer to accept a `embedding_requires_grad` and its value will be used when setting the property to the `Embedding` layer. However, I am going with the former solution to have a clear distinction between `CNNStatic` and `CNNNonStatic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNNonStatic(CNNStatic):\n",
    "    def __init__(self, embedding_weights, num_vocab, embedding_dim, channel_out, filter_sizes, num_output, dropout_prob):\n",
    "        super(CNNNonStatic, self).__init__(embedding_weights, num_vocab, embedding_dim, channel_out, filter_sizes, num_output, dropout_prob)\n",
    "        \n",
    "        self.embedding.requires_grad = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f272a3065747999add66dcf2865a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 1] Loss: 0.46\n",
      "[Training - Epoch 1] Accuracy: 0.77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687e0eeb3ce24debb2b045d282a60084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 1] Loss: 0.32\n",
      "[Validation - Epoch 1] Accuracy: 0.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c789efff2d6c45d8ba9f8ec8bf1684f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 2] Loss: 0.27\n",
      "[Training - Epoch 2] Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7bfab2711a46368d7094879851e512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 2] Loss: 0.28\n",
      "[Validation - Epoch 2] Accuracy: 0.88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3990684a214ccf977452e60f057253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 3] Loss: 0.17\n",
      "[Training - Epoch 3] Accuracy: 0.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe6aa8475734488beede441a12dbaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 3] Loss: 0.26\n",
      "[Validation - Epoch 3] Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ecfd6005364069bf67af912456f17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 4] Loss: 0.09\n",
      "[Training - Epoch 4] Accuracy: 0.97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a068e1df35714a36b8760cb34f139692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 4] Loss: 0.29\n",
      "[Validation - Epoch 4] Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167fde3177b040f8b44476b2686d3cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 5] Loss: 0.04\n",
      "[Training - Epoch 5] Accuracy: 0.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8fdfdd1061b40a5a22364e94beb054f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 5] Loss: 0.32\n",
      "[Validation - Epoch 5] Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2899bdcbd45149ad9df0f4cd70223851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test] Loss: 0.28\n",
      "[Test] Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "cnn_non_static = CNNNonStatic(*args)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cnn_non_static.parameters())\n",
    "\n",
    "train_evaluate_model(cnn_non_static, loss_function, optimizer, training_dataset, test_dataset, validation_dataset=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 5 epochs, the test set accuracy of this model is 88% on the test set. There's a 1% decrease in the accuracy from the `CNNStatic` model. Next, I will create a 2-channel embedding, and see observe its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: CNNMultiChannel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model architecture, two channels are used as input to the convolutional layer. Both channels are initialized using the pre-trained word embedding. However, one of those channels will have its parameters learned during training while the other does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNMultiChannel(nn.Module):\n",
    "    def __init__(self, embedding_weights, num_vocab, embedding_dim, channel_out, filter_sizes, num_output, dropout_prob):\n",
    "        super(CNNMultiChannel, self).__init__()\n",
    "        \n",
    "        self.non_static_embedding = nn.Embedding(num_vocab, embedding_dim)\n",
    "        self.non_static_embedding.weight.data.copy_(embedding_weights)\n",
    "        \n",
    "        self.static_embedding = nn.Embedding(num_vocab, embedding_dim)\n",
    "        self.static_embedding.weight.data.copy_(embedding_weights)\n",
    "        self.static_embedding.requires_grad = False\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv2d(2, channel_out, filter_sizes[i]) for i in range(len(filter_sizes))])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * channel_out, num_output)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.t()\n",
    "        \n",
    "        x_non_static_embed = self.non_static_embedding(x).unsqueeze(1)\n",
    "        x_static_embed = self.static_embedding(x).unsqueeze(1)\n",
    "        x_embed = torch.cat((x_non_static_embed, x_static_embed), 1)\n",
    "        x_convs = [F.relu(conv(x_embed)).squeeze(3) for conv in self.convs]\n",
    "        x_pools = [F.max_pool1d(x_conv, x_conv.size(2)).squeeze(2) for x_conv in x_convs]\n",
    "        x_cat = self.dropout(torch.cat(x_pools, 1))\n",
    "        x_fc = self.fc(x_cat).squeeze(1)\n",
    "        \n",
    "        return x_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ea2c45d6de4511883936fba4d31f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 1] Loss: 0.44\n",
      "[Training - Epoch 1] Accuracy: 0.79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfcadf364704f379ffae33ac22c17a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 1] Loss: 0.30\n",
      "[Validation - Epoch 1] Accuracy: 0.87\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8036e11fd2b4848bbc6cff05936029c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 2] Loss: 0.27\n",
      "[Training - Epoch 2] Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82eae3b15a344459793a540649cd8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 2] Loss: 0.26\n",
      "[Validation - Epoch 2] Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f85ad19c1da4657938209dcc786bb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 3] Loss: 0.16\n",
      "[Training - Epoch 3] Accuracy: 0.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9d579625d64a5a8bb9d089bc13bebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 3] Loss: 0.26\n",
      "[Validation - Epoch 3] Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8017b5c6bf54c128f6a8153a57d9b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 4] Loss: 0.08\n",
      "[Training - Epoch 4] Accuracy: 0.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e9d7af6c3f496183e64d1f7db5c9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 4] Loss: 0.29\n",
      "[Validation - Epoch 4] Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c0a90022324868971cbb5c39396994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training - Epoch 5] Loss: 0.03\n",
      "[Training - Epoch 5] Accuracy: 0.99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f209a72f5c2428fb7092fbab112959c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Validation - Epoch 5] Loss: 0.34\n",
      "[Validation - Epoch 5] Accuracy: 0.88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620eb219ffb3456090d4745b46968289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Test] Loss: 0.29\n",
      "[Test] Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "cnn_multi_channel = CNNMultiChannel(*args)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(cnn_multi_channel.parameters())\n",
    "\n",
    "train_evaluate_model(cnn_multi_channel, loss_function, optimizer, training_dataset, test_dataset, validation_dataset=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 5 epochs, this model has achieved a test set accuracy of 88%. As in Kim's paper, they have not seen much improvement with the usage of static and non-static channels. The hope is that the static channel will help the non-static channel not deviate too far from the information already present in the pre-trained word embedding, at the same time learn specific relationships of words with regards to movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through Kim's paper about the usage of convolutional neural networks in sentiment analysis has shown potential to be used as one of the standard architectures in this field, alongside with recurrent neural networks. Overall, `CNNStatic`, `CNNNonStatic`, and `CNNMultiChannel` has shown similar test set accuracies. More hyperparameter adjustments could possibly increase an already good accuracy for sentiment analysis with IMDB movie reviews dataset.\n",
    "\n",
    "Furthermore, I have learned a lot of using the PyTorch framework to build this (and thanks to [this](https://github.com/bentrevett/pytorch-sentiment-analysis) reference for guiding me through). Lastly, this project has taught me how to write more streamlined functions to train and evaluate models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
